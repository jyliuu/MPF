{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " # Hyperparameter Search for MPF Boosted Model\n",
                "\n",
                "\n",
                "\n",
                " In this cell we define helper functions for evaluating one hyperparameter candidate and performing a randomized search in parallel. We then use our random search function on training data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import mpf_py\n",
                "from utils import gen_data, true_model3, plot_2d_model_predictions  # Adjust import according to your project structure\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " ### Example Usage for MPF\n",
                "\n",
                "\n",
                "\n",
                " Here we generate training data (using a hypothetical `gen_data` and `true_model3` from your utils) and run the random hyperparameter search."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate data\n",
                "true_model = lambda x: 2*x[:,1] + x[:,0] - 0.5 * x[:,0]* x[:,1] + 34\n",
                "\n",
                "x, y = gen_data(n=10000, seed=3, model=true_model3)\n",
                "x_train = x[:5000]\n",
                "y_train = y[:5000]\n",
                "\n",
                "x_test = x[5000:]\n",
                "y_test = y[5000:]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fit the best MPF model\n",
                "from utils import random_hyperparam_search_parallel\n",
                "\n",
                "\n",
                "best_model, best_fr, best_params, best_error = random_hyperparam_search_parallel(\n",
                "    x_train, y_train, n_splits=2, n_candidates=50, n_jobs=3, param_distributions= {\n",
                "        \"epochs\": lambda: randint(2, 9).rvs(),       # 1 to 8 inclusive\n",
                "        \"n_iter\": lambda: randint(5, 101).rvs(),         # 5 to 100 inclusive\n",
                "        \"split_try\": lambda: randint(5, 21).rvs(),         # 5 to 20 inclusive\n",
                "        \"B\": lambda: randint(10, 101).rvs(),         # 10 to 100 inclusive\n",
                "        \"colsample_bytree\": lambda: 1.0,\n",
                "        \"identified\": lambda: False\n",
                "    })\n",
                "print(\"Best hyperparameters for MPF:\", best_params)\n",
                "print(\"Best CV MSE for MPF:\", best_error)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_params, best_error\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " # Hyperparameter Search for XGBoost\n",
                "\n",
                "\n",
                "\n",
                " In this section we use scikit‑learn’s RandomizedSearchCV with continuous and discrete ranges. We fix 2‑fold (or 4‑fold as set below) cross‑validation and use random sampling over the following ranges:\n",
                "\n",
                "\n",
                "\n",
                " - `max_depth`: integers from 3 to 9,\n",
                "\n",
                " - `learning_rate`: continuous values in [0.001, 0.6],\n",
                "\n",
                " - `n_estimators`: integers from 200 to 800.\n",
                "\n",
                "\n",
                "\n",
                " We then print the best hyperparameters and CV MSE, and retrieve the best model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "from xgboost import XGBRegressor\n",
                "from sklearn.model_selection import RandomizedSearchCV\n",
                "from scipy.stats import uniform, randint\n",
                "\n",
                "# Define hyperparameter distributions.\n",
                "param_distributions = {\n",
                "    'max_depth': randint(3, 10),           # Integers from 3 to 9.\n",
                "    'learning_rate': uniform(0.001, 0.599),  # Continuous values in [0.001, 0.6].\n",
                "    'n_estimators': randint(200, 801)        # Integers from 200 to 800.\n",
                "}\n",
                "\n",
                "# Create an XGBRegressor.\n",
                "xgb_model = XGBRegressor(random_state=42)\n",
                "\n",
                "# Set up RandomizedSearchCV with 4-fold CV.\n",
                "random_search = RandomizedSearchCV(\n",
                "    estimator=xgb_model,\n",
                "    param_distributions=param_distributions,\n",
                "    n_iter=100,   # number of random candidates to try\n",
                "    scoring='neg_mean_squared_error',\n",
                "    cv=4,         # use 4-fold cross-validation (adjust as needed)\n",
                "    n_jobs=-1,    # use all available cores\n",
                "    verbose=1,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "# Run the randomized hyperparameter search.\n",
                "random_search.fit(x_train, y_train)\n",
                "print(\"Best xgboost hyperparameters:\", random_search.best_params_)\n",
                "print(\"Best xgboost CV MSE:\", -random_search.best_score_)\n",
                "\n",
                "best_model_xgboost = random_search.best_estimator_\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " # Comparing Models\n",
                "\n",
                "\n",
                "\n",
                " Finally, we plot the true model, the best MPF model, and the best XGBoost model using a provided plotting function `plot_model_predictions`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Assuming plot_model_predictions is defined in your environment\n",
                "plot_2d_model_predictions(true_model3, title=\"True model\")\n",
                "plot_2d_model_predictions(lambda x: best_model.predict(x), title=\"Best MPF model\")\n",
                "plot_2d_model_predictions(lambda x: best_model_xgboost.predict(x), title=\"Best XGBoost model\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the best MPF model with identified=True\n",
                "plot_2d_model_predictions(lambda x: best_model.predict(x), title=\"Best MPF model with identified=True\")\n",
                "test_preds = best_model.predict(x_test)\n",
                "test_error = np.mean((y_test - test_preds) ** 2)\n",
                "print(f\"Test MSE for MPF with identified=True: {test_error}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "mpf_tree_grid_rep = [mpf_py.TreeGrid(tgf.combined_tree_grid) for tgf in best_model.tree_grid_families]\n",
                "\n",
                "pred_function = lambda x: sum([tg.predict(x) for tg in mpf_tree_grid_rep])\n",
                "\n",
                "plot_2d_model_predictions(pred_function, title=\"Identified MPF model predictions\")\n",
                "test_preds_identified = pred_function(x_test)\n",
                "test_error_identified = np.mean((y_test - test_preds_identified) ** 2)\n",
                "print(f\"Test MSE for MPF with identified=True: {test_error_identified}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "for tg in mpf_tree_grid_rep:\n",
                "    tg.plot_components()\n",
                "    plot_2d_model_predictions(lambda x: tg.predict(x), title=f\"TG scaled: {tg.scaling}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
